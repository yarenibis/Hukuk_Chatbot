# -*- coding: utf-8 -*-
import os
import re
import sys
import traceback
import torch
import streamlit as st
import pandas as pd
from dotenv import load_dotenv
from datasets import load_dataset
# LangChain: Embedding + Vector Store
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain.schema import Document
from langchain_community.embeddings import HuggingFaceEmbeddings
# Transformers
from transformers import AutoTokenizer, AutoModelForCausalLM



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 0ï¸âƒ£ Ortam DeÄŸiÅŸkenleri
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()
HF_TOKEN = os.getenv("HUGGINGFACEHUB_API_TOKEN")

APP_TITLE = "âš–ï¸ TÃ¼rkÃ§e Hukuk Chatbotu"
APP_CAPTION = "TÃ¼rkÃ§e hukuk sorularÄ±nÄ± Qwen3-1.7B modeliyle yanÄ±tlar."

# Hugging Faceâ€™ten alÄ±nan TÃ¼rkÃ§e hukuk veri seti
DEFAULT_DATASET = "alibayram/hukuk_soru_cevap"
DEFAULT_SPLIT = "train"

# Embedding ve LLM model bilgileri
EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
LOCAL_LLM_MODEL = "Qwen/Qwen3-1.7B"
CHROMA_DIR = "chroma_hukuk_db"

# RAG parametreleri
RETRIEVER_K = 6
CHUNK_SIZE = 1200
CHUNK_OVERLAP = 100


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1ï¸âƒ£ Dataset ve Chroma VektÃ¶r VeritabanÄ± HazÄ±rlÄ±ÄŸÄ±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(show_spinner=False)
def load_hf_df(dataset_name: str, split: str) -> pd.DataFrame:
    """
       Hugging Face Ã¼zerindeki veri setini yÃ¼kler ve 'soru' ile 'cevap' sÃ¼tunlarÄ±nÄ± dÃ¼zenler.
    """
    ds = load_dataset(dataset_name, split=split, token=HF_TOKEN)
    df = ds.to_pandas()
    df = df.rename(columns={c.lower(): c for c in df.columns})
    if "soru" not in df.columns or "cevap" not in df.columns:
        raise ValueError("Dataset'te 'soru' veya 'cevap' kolonu yok.")
    df["soru"] = df["soru"].astype(str).str.strip()
    df["cevap"] = df["cevap"].astype(str).str.strip()
    return df.dropna(subset=["soru", "cevap"]).reset_index(drop=True)


@st.cache_resource(show_spinner=False)
def build_or_load_chroma(df: pd.DataFrame, persist_dir: str):
    """
        Chroma vektÃ¶r veritabanÄ±nÄ± oluÅŸturur veya mevcutsa yÃ¼kler.
        Embedding modeli olarak Ã§ok dilli MiniLM kullanÄ±lÄ±r.
    """
    os.makedirs(persist_dir, exist_ok=True)
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
    # EÄŸer zaten oluÅŸturulmuÅŸ bir Chroma veritabanÄ± varsa onu yÃ¼kler
    if any(f.endswith(".sqlite3") for f in os.listdir(persist_dir)):
        return Chroma(persist_directory=persist_dir, embedding_function=embeddings)
    # Yeni vektÃ¶r veritabanÄ± oluÅŸturma sÃ¼reci
    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
    docs = [Document(page_content=f"Soru: {r['soru']}\n\nCevap: {r['cevap']}") for _, r in df.iterrows()]
    chunks = [Document(page_content=c) for d in docs for c in splitter.split_text(d.page_content)]
    return Chroma.from_documents(chunks, embedding=embeddings, persist_directory=persist_dir)



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2ï¸âƒ£ Qwen3-1.7B Modeli
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource(show_spinner=False)
def load_qwen_model(model_name=LOCAL_LLM_MODEL):
    """Qwen3-1.7B chat modelini yÃ¼kler (CPU uyumlu)."""
    torch.set_num_threads(max(1, os.cpu_count() // 2))
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32).to("cpu").eval()
    return tokenizer, model



# ğŸ”¹ Qwen modellerinin reasoning (<think>) kÄ±smÄ±nÄ± temizlemek iÃ§in yardÄ±mcÄ± fonksiyon
def clean_output(text: str) -> str:
    """
    Modelin Ã¼rettiÄŸi <think> bÃ¶lÃ¼mlerini temizler (iÃ§sel dÃ¼ÅŸÃ¼nceleri gizler).
    """
    # <think> ile baÅŸlayan tÃ¼m bÃ¶lÃ¼mleri sil
    text = re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL)
    text = re.sub(r"<think>.*", "", text, flags=re.DOTALL)
    # BoÅŸluklarÄ± dÃ¼zelt
    return text.strip()



# ğŸ”¹ KullanÄ±cÄ± mesajÄ±nÄ± Qwen chat formatÄ±na uygun hale getirir
def create_messages(question, context):
    """
        Qwen chat formatÄ±na uygun ÅŸekilde system + user mesajlarÄ± oluÅŸturur.
        TÃ¼rkÃ§e cevaplarÄ± zorunlu kÄ±lar ve modelin iÃ§sel dÃ¼ÅŸÃ¼nce Ã¼retmesini engeller.
    """
    messages = [
        {
            "role": "system",
            "content": (
                "Sen bir TÃœRK hukuk danÄ±ÅŸmanÄ±sÄ±n. "
                "CevaplarÄ±nÄ± her zaman TÃœRKÃ‡E olarak ver. "
                "Ä°ngilizce veya baÅŸka bir dilde asla cevap verme. "
                "Asla iÃ§sel dÃ¼ÅŸÃ¼nce (<think>), aÃ§Ä±klama veya adÄ±m adÄ±m mantÄ±k yazma. "
                "Sadece nihai cevabÄ± yaz. "
                "KullanÄ±cÄ±ya yalnÄ±zca verilen baÄŸlamdaki bilgilere dayanarak kÄ±sa, resmi ve doÄŸru bir TÃœRKÃ‡E hukuk cevabÄ± ver. "
                "Yorum yapma, tahmin etme, gereksiz tekrar etme."
                "Cevap en fazla 2-3 paragraf uzunluÄŸunda olsun."
            ),
        },
        {
            "role": "user",
            "content": (
                f"Bu soru TÃ¼rkÃ§e yazÄ±lmÄ±ÅŸtÄ±r. LÃ¼tfen yanÄ±tÄ± da TÃœRKÃ‡E ver.\n\n"
                f"BAÄLAM:\n{context}\n\nSORU:\n{question}"
            ),
        },
    ]
    return messages


# ğŸ”¹ Modelden yanÄ±t Ã¼retme fonksiyonu
def generate_answer(model, tokenizer, question, context, max_new_tokens=400):
    """
    Soru + baÄŸlam bilgisini alÄ±r, modelden yanÄ±t Ã¼retir ve <think> kÄ±smÄ±nÄ± temizler.
    """
    messages = create_messages(question, context)
    # Qwen modelleri iÃ§in doÄŸru encoding:
    text_input = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        return_tensors="pt"
    )
    # BazÄ± modeller dict yerine direkt tensor dÃ¶ndÃ¼rÃ¼r â†’ kontrol ediyoruz
    if isinstance(text_input, torch.Tensor):
        inputs = {"input_ids": text_input.to(model.device)}
    else:
        inputs = text_input.to(model.device)
    # Modelden yanÄ±t Ã¼retimi (temperature dÃ¼ÅŸÃ¼k = daha kararlÄ± TÃ¼rkÃ§e cevap)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.4,
            top_p=0.9,
            repetition_penalty=1.2,
            no_repeat_ngram_size=4
        )
    # YalnÄ±zca yeni Ã¼retilen kÄ±smÄ± Ã§Ã¶zÃ¼yoruz
    input_length = inputs["input_ids"].shape[-1]
    text = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True)
    text = clean_output(text)  # ğŸ’¥ yeni satÄ±r
    return text.strip() or "YanÄ±t Ã¼retilemedi."



# ğŸ”¹ BaÄŸlamÄ± (context) belirli uzunlukta birleÅŸtirir
def build_context(docs, max_len=6000):
    """
    Arama sonucu dÃ¶nen belgeleri birleÅŸtirip modelin anlayacaÄŸÄ± ÅŸekilde bir context oluÅŸturur.
    """
    buf, total = [], 0
    for d in docs:
        if total + len(d.page_content) > max_len:
            break
        buf.append(d.page_content)
        total += len(d.page_content)
    return "\n\n".join(buf)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3ï¸âƒ£ Streamlit ArayÃ¼zÃ¼
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    # Sayfa baÅŸlÄ±ÄŸÄ± ve ikon
    st.set_page_config(page_title=APP_TITLE, page_icon="âš–ï¸")
    st.title(APP_TITLE)
    st.caption(APP_CAPTION)

    # Dataset yÃ¼kleniyor
    with st.spinner("ğŸ“‚ Dataset yÃ¼kleniyor..."):
        df = load_hf_df(DEFAULT_DATASET, DEFAULT_SPLIT)

    # Chroma veritabanÄ± hazÄ±rlanÄ±yor
    with st.spinner("ğŸ“š Chroma veritabanÄ± hazÄ±rlanÄ±yor..."):
        vectordb = build_or_load_chroma(df, CHROMA_DIR)
    retriever = vectordb.as_retriever(search_kwargs={"k": RETRIEVER_K})
    # Model yÃ¼kleniyor
    with st.spinner(f"ğŸ¤– Model yÃ¼kleniyor ({LOCAL_LLM_MODEL})..."):
        tokenizer, model = load_qwen_model(LOCAL_LLM_MODEL)
    # Sohbet geÃ§miÅŸi yÃ¶netimi
    if "messages" not in st.session_state:
        st.session_state.messages = []
    # Eski mesajlarÄ± arayÃ¼zde gÃ¶ster
    for m in st.session_state.messages:
        with st.chat_message(m["role"]):
            st.markdown(m["content"])
    # KullanÄ±cÄ±dan yeni soru al
    question = st.chat_input("Bir hukuk sorusu yazÄ±nâ€¦ (Ã¶rnek:... Nafaka hakkÄ±m var mÄ±?)")
    if question:
        # KullanÄ±cÄ± mesajÄ±nÄ± ekrana yaz
        st.session_state.messages.append({"role": "user", "content": question})
        with st.chat_message("user"):
            st.markdown(question)
        # Belgelerden baÄŸlam oluÅŸtur ve modelden yanÄ±t al
        with st.spinner("ğŸ§  Belgeler aranÄ±yor ve yanÄ±t hazÄ±rlanÄ±yor..."):
            docs = retriever.invoke(question)
            context = build_context(docs)
            answer = generate_answer(model, tokenizer, question, context)
        # Asistan cevabÄ±nÄ± ekrana yaz
        st.session_state.messages.append({"role": "assistant", "content": answer})
        with st.chat_message("assistant"):
            st.markdown(answer)


if __name__ == "__main__":
    try:
        main()
    except Exception:
        traceback.print_exc(file=sys.stderr)
